# VellumForge2

**VellumForge2** is a highly configurable and easy to use open-source command-line tool for synthetically generating high-quality **Direct Preference Optimization (DPO)** datasets using Large Language Models (LLMs) written in Golang. It implements a hierarchical generation pipeline with optional LLM-as-a-Judge evaluation to create preference pairs suitable for fine-tuning language models.

## Key Features

### **Hierarchical Generation Pipeline**
- Generate subtopics from a main theme
- Create diverse prompts for each subtopic
- Produce preference pairs (chosen/rejected responses) with customizable story generation templates
- **Smart over-generation strategy** for 95%+ count accuracy (requests 115% of target, deduplicates, single retry if needed)

### **Provider-Agnostic API Support**
- Works with any OpenAI-compatible API endpoint
- Supports OpenAI, NVIDIA, Anthropic, Together AI, local servers (llama.cpp, kobold.cpp, Ollama, LM Studio, etc)
- Enhanced rate limit handling with exponential backoff (6s, 18s, 54s for 429 errors)

### **LLM-as-a-Judge Evaluation** (Optional)
- Automated quality assessment with configurable rubrics
- Structured scoring across multiple criteria
- "One-to-many" hybrid schema with score totals and detailed per-criterion reasoning
- JSON sanitization to handle unescaped newlines from LLM responses
- Preference margin calculation for advanced DPO training

### **High Performance & Reliability**
- Concurrent worker pool for parallel API requests
- Per-model rate limiting with token bucket algorithm
- Intelligent retry logic for rate limits (3^n backoff)
- **Pre-validation layer** prevents JSON parse failures (99%+ success rate)
- Robust JSON extraction with proper bracket matching
- Auto-fixes truncated JSON responses
- Case-insensitive deduplication of generated items
- **Checkpoint/resume** for interrupted sessions with async I/O

### **Rich Output & Logging**
- JSONL format compatible with DPO training frameworks
- Session-based organization with ISO 8601 timestamped directories (`session_YYYY-MM-DDTHH-MM-SS`)
- Dual logging - JSON to file (`session.log`) + text to stdout
- Comprehensive error logging with full response context
- Generation statistics with failure rate tracking
- Checkpoint files for session state persistence

### **Hugging Face Hub Integration**
- Native NDJSON commit API implementation (no external dependencies)
- One-command dataset uploads
- Automatic repository creation
- Selective file uploads (dataset + config only, excludes logs)
- Uploads config as `vf2.toml` to Hub for clarity

#### Check out the [Changelog](CHANGELOG.md) for all new features

## Installation

### Prebuilt Binaries

If you don't want to compile from source binaries are built for all platforms (Windows, Linux, MacOS x86_64 and MacOS arm64) automatically by github workflows, triggered by new release tags. 

It's a good idea to also download the example configs to help get started.

If you have issues with the latest release try downloading binaries for an older version. 

### From Source (Recommended)

```bash
# Clone the repository
git clone https://github.com/lemon07r/vellumforge2.git
cd vellumforge2

# Install dependencies
make install

# Build the binary
make build

# The binary will be in ./bin/vellumforge2
```

### Cross-Platform Builds

```bash
# Build for all platforms
make build-all

# Or build for specific platforms
make build-linux
make build-darwin
make build-windows
```

## Quick Start

Also check out the [Getting Started Guide](GETTING_STARTED.md)

### 1. Set Up Configuration

```bash
# Copy example files
cp configs/config.example.toml config.toml
cp configs/.env.example .env

# Edit config.toml with your desired settings
# RECOMMENDED: Use the Even Better TOML VSCode extension
# Edit .env with your API keys
```

### 2. Run Generation

```bash
# Basic usage
./bin/vellumforge2 run --config config.toml

# With verbose logging (recommended for debugging)
./bin/vellumforge2 run --config config.toml --verbose

# With Hugging Face upload
./bin/vellumforge2 run --config config.toml --upload-to-hf --hf-repo-id username/my-dataset
```

### 3. Results

All outputs are saved in a timestamped session directory:

```
output/
└── session_2025-10-27T12-34-56/
    ├── dataset.jsonl       # Your DPO dataset
    ├── config.toml.bak     # Configuration snapshot
    └── session.log         # Structured JSON logs
```

## Example Datasets

Check out some datasets generated by this tool here:

https://huggingface.co/collections/lemon07r/vellumforge2-datasets

Configs used to generate these datasets can be found in the included vf2.toml under the files tab.

This collection currently includes:

- lemon07r/VellumK2-Fantasy-DPO-Tiny-01 - **A 128 row dataset that can be useful for testing or validation**
- lemon07r/VellumK2-Fantasy-DPO-Small-01 - **A 1k row dataset that can be useful light training or in combination with other datasets**
- lemon07r/VellumK2-Fantasy-DPO-01 (planned) - **A 10k row large dataset perfect for training with or without other datasets**

These are all generated using Kimi-K2-0905 from Nvidia NIM API for topics, prompts, chosen responses and judge scoring. We use Phi-4-mini-instruct (Q6K from unsloth) run locally over llama.cpp server w/ ROCm (on a 6700 XT) for rejected responses. All of these datasets can be used for SFT just using prompt + chosen response columns, DPO using prompt + the chosen/rejected pairs, reward training using the score totals, and/or MORL training using the judge responses. See the dataset format section of the README.md further down below for more information. 

## Configuration

### Example Configuration (config.toml)

```toml
[generation]
main_topic = "Fantasy Fiction"
num_subtopics = 64
num_prompts_per_subtopic = 2
concurrency = 8  # Number of parallel workers (max: 1024)
over_generation_buffer = 0.15  # Request 15% extra to hit target counts
max_exclusion_list_size = 50  # Limit retry prompt size
# disable_validation_limits = false  # Set true to exceed default limits

[models.main]
base_url = "https://integrate.api.nvidia.com/v1"
model_name = "moonshotai/kimi-k2-instruct-0905"
temperature = 0.7
max_output_tokens = 16384
context_size = 262144
rate_limit_per_minute = 40  # Respects API quotas

[models.rejected]
base_url = "https://integrate.api.nvidia.com/v1"
model_name = "meta/llama-3.1-8b-instruct"
temperature = 1.0  # Higher temp for lower quality responses
max_output_tokens = 16384
rate_limit_per_minute = 40

[models.judge]
enabled = true  # Set to false to disable judge evaluation
base_url = "https://integrate.api.nvidia.com/v1"
model_name = "moonshotai/kimi-k2-instruct-0905"
temperature = 0.4  # Lower temp for consistent evaluation
max_output_tokens = 16384
rate_limit_per_minute = 40

[prompt_templates]
# Generation templates are REQUIRED
chosen_generation = '''You are a talented creative writer.
Write a compelling story (400-600 words) based on this prompt:

{{.Prompt}}

Your story should have vivid descriptions and engaging characters.'''

rejected_generation = '''Write a story based on this prompt:

{{.Prompt}}

Write 300-400 words.'''

# Judge rubric template (optional if judge disabled), this increase dataset generation time
judge_rubric = '''Evaluate the following story according to multiple criteria...'''
```

### Environment Variables (.env)

```bash
NVIDIA_API_KEY=nvapi-xxxxx
OPENAI_API_KEY=sk-xxxxx
HUGGING_FACE_TOKEN=hf_xxxxx
```

## Dataset Format

### One-to-Many Hybrid Schema

VellumForge2 uses a flexible "one-to-many" hybrid schema that works seamlessly with:
- **DPOTrainer**: Use `prompt`, `chosen`, `rejected` columns
- **RewardTrainer**: Use texts with `chosen_score_total` and `rejected_score_total` as labels
- **Custom MORL Training**: Parse nested `chosen_scores` and `rejected_scores` for multi-objective training

Using a judge is optional, generate datasets without a judge for DPO only training. Use without rejected responses for SFT training.

Each line in `dataset.jsonl` is a JSON object:

```json
{
  "main_topic": "Fantasy Fiction",
  "sub_topic": "Dragon Lore",
  "prompt": "Write a story about a dragon who is afraid of heights.",
  "chosen": "Once upon a time, in a cavern of glittering obsidian...",
  "rejected": "There was a dragon. It was big and green...",
  
  "chosen_scores": {
    "plot_and_structural_integrity": {
      "score": 5,
      "reasoning": "Excellent narrative arc with clear causality..."
    },
    "creativity_and_originality": {
      "score": 4,
      "reasoning": "Fresh take on the dragon archetype..."
    }
  },
  "rejected_scores": {
    "plot_and_structural_integrity": {
      "score": 2,
      "reasoning": "The plot is disjointed and lacks coherence..."
    },
    "creativity_and_originality": {
      "score": 2,
      "reasoning": "Relies heavily on standard clichés..."
    }
  },
  
  "chosen_score_total": 4.5,
  "rejected_score_total": 2.0,
  "preference_margin": 2.5
}
```

**No data transformation needed** to switch between training frameworks

## Advanced Usage

### Custom Prompt and Response Generation Templates

Customize the entire generation pipeline by editing templates in `config.toml`:

```toml
[prompt_templates]

# Subtopic generation (with optional retry support)
subtopic_generation = '''Generate {{.NumSubtopics}} specific subtopics for: {{.MainTopic}}

{{if .IsRetry}}NOTE: Avoid these already generated: {{.ExcludeSubtopics}}
{{end}}
Return ONLY a JSON array of subtopics.'''

# Prompt generation  
prompt_generation = '''Generate {{.NumPrompts}} creative writing prompts for: {{.SubTopic}}

Context: This is part of "{{.MainTopic}}"
Return ONLY a JSON array of prompts.'''

# Response generation for chosen responses
chosen_generation = '''You are an expert {{.MainTopic}} writer.

Write a masterful story for this prompt:
{{.Prompt}}

Requirements:
- 500-700 words
- Rich world-building
- Complex characters
- Surprising twist ending'''

# Response generation for rejected responses
rejected_generation = '''Write a basic {{.MainTopic}} story:
{{.Prompt}}

Keep it simple, 200-300 words.'''

# Judge evaluation (optional)
judge_rubric = '''Evaluate the following story...'''
```

**Available Template Variables:**

- **Subtopic Generation:**
  - `{{.MainTopic}}` - Your main theme
  - `{{.NumSubtopics}}` - Count to generate (auto-adjusted for over-generation and retry)
  - `{{.IsRetry}}` - Boolean, true on retry attempts (optional)
  - `{{.ExcludeSubtopics}}` - Comma-separated list of already generated subtopics (optional, only on retry)
  
- **Prompt Generation:**
  - `{{.SubTopic}}` - The current subtopic
  - `{{.NumPrompts}}` - Number of prompts to generate
  - `{{.MainTopic}}` - Main topic (also available here)

- **Story Generation (chosen_generation, rejected_generation):**
  - `{{.Prompt}}` - The writing prompt
  - `{{.MainTopic}}` - Main topic
  - `{{.SubTopic}}` - Current subtopic

- **Judge Evaluation:**
  - `{{.Prompt}}` - Original writing prompt
  - `{{.StoryText}}` - Story to evaluate

### Judge Evaluation Criteria

Customize evaluation criteria with your own rubric:

```toml
[prompt_templates]
judge_rubric = '''Evaluate the following story on these criteria:

STORY:
{{.StoryText}}

Rate each criterion 1-5 and provide reasoning:

Return ONLY valid JSON:
{
  "plot_quality": {"score": 1-5, "reasoning": "Detailed analysis..."},
  "character_depth": {"score": 1-5, "reasoning": "Detailed analysis..."},
  "writing_style": {"score": 1-5, "reasoning": "Detailed analysis..."}
}
'''
```

**Important**: Judge is flexible - it accepts any criteria the model returns!

### Rate Limiting Strategy

Configure per-model rate limits to respect API quotas and avoid 429 errors:

```toml
[models.main]
rate_limit_per_minute = 40  # 40 requests per minute

[generation]
concurrency = 8  # 8 parallel workers
```

**Rate Limit Math**: With 8 workers at 40 req/min, you're making ~5 req/min per worker on average.

**Smart Retry Logic**: VellumForge2 automatically applies longer exponential backoff for rate limit errors (6s → 18s → 54s), capped at 2 minutes by default.

## Configuration Best Practices

### Over-Generation Strategy

VellumForge2 uses an intelligent over-generation strategy to reliably hit target counts:

```toml
[generation]
over_generation_buffer = 0.15  # Request 15% extra (configurable: 0.0-1.0)
max_exclusion_list_size = 50   # Limit retry prompt size
```

**How it works:**
1. Request `target × (1 + buffer)` items initially (e.g., 115 for target=100)
2. Deduplicate results (case-insensitive)
3. If short, retry once for the difference with exclusion list
4. Achieves 95%+ accuracy vs 79% with single-shot

**Tuning tips:**
- Higher buffer (0.25-0.50) for models that frequently undershoot
- Lower buffer (0.05-0.10) for reliable models to save API calls
- Set to 0.0 to disable (not recommended)

### Rate Limiting and Backoff

VellumForge2 automatically handles rate limits with intelligent backoff:

- **Standard retries**: 2^n exponential backoff (2s, 4s, 8s...)
- **Rate limit retries**: 3^n exponential backoff (6s, 18s, 54s...)
- **Automatic cap**: Maximum 2 minutes per retry (configurable)

```toml
[models.main]
rate_limit_per_minute = 40
max_backoff_seconds = 120  # Optional: override default 2-minute cap
```

**Tuning tips:**
- Reduce `max_backoff_seconds` for faster failure detection
- Increase for heavily rate-limited APIs (up to 300s / 5 minutes)
- Judge models may need longer caps due to complex responses

### Safe Configuration Limits

VellumForge2 enforces safety limits to prevent resource exhaustion:

| Config Field | Min | Max | Recommended |
|--------------|-----|-----|-------------|
| `concurrency` | 1 | 1024* | 4-16 |
| `num_subtopics` | 1 | 10,000* | 10-500 |
| `num_prompts_per_subtopic` | 1 | 10,000* | 2-10 |
| `over_generation_buffer` | 0.0 | 1.0 | 0.15 |
| `max_output_tokens` | 1 | `context_size` | Model-specific |

\* _Limits can be disabled by setting `disable_validation_limits = true` (use with caution)_

**Validation**: The config validator checks these limits and provides clear error messages.

**Disabling Limits**: For extreme use cases, you can disable upper bound validation:

```toml
[generation]
disable_validation_limits = true  # USE WITH CAUTION
concurrency = 2048  # Now allowed
num_subtopics = 50000  # Now allowed
```

**Warning**: Disabling limits may cause memory exhaustion or API rate limit issues. Only use when you have sufficient resources and understand the implications.

### Graceful Shutdown

VellumForge2 v1.2+ supports graceful shutdown via Ctrl+C:

- Press **Ctrl+C** once to initiate graceful shutdown
- Current batch completes, then stops cleanly
- Partial dataset is saved to session directory
- Logs show "Generation cancelled by user"

**Use cases:**
- Stop long-running generations early
- Adjust configuration and restart
- Respond to resource constraints

### Checkpoint & Resume

VellumForge2 v1.3+ includes robust checkpoint/resume functionality for interrupted sessions:

**Features:**
- Automatic state persistence during generation
- Resume from any interruption (Ctrl+C, crash, system failure)
- Phase-based checkpointing (subtopics, prompts, preference pairs)
- Async checkpoint writes for minimal performance impact (<1% overhead)
- Config validation to prevent incompatible resumes

**Enable checkpointing:**

```toml
[generation]
enable_checkpointing = true
checkpoint_interval = 10  # Save every 10 completed jobs
```

**Resume after interruption:**

```bash
# Option 1: Edit config
[generation]
resume_from_session = "session_2025-10-28T14-30-00"

# Then run normally
./bin/vellumforge2 run --config config.toml

# Option 2: Use CLI command (auto-updates config)
./bin/vellumforge2 checkpoint resume session_2025-10-28T14-30-00
```

**Manage checkpoints:**

```bash
# List all sessions with checkpoint status
./bin/vellumforge2 checkpoint list

# Inspect checkpoint details
./bin/vellumforge2 checkpoint inspect session_2025-10-28T14-30-00
```

**What's saved:**
- All completed subtopics
- All generated prompts
- Each completed preference pair (job)
- Cumulative statistics
- Current phase and progress

**Checkpoint file structure:**

```
output/
└── session_2025-10-28T14-30-00/
    ├── dataset.jsonl       # Incremental results
    ├── checkpoint.json     # State for resume
    ├── config.toml.bak     # Config snapshot
    └── session.log         # Structured logs
```

**Use cases:**
- Long-running generations (10K+ rows)
- Unstable network connections
- Experimenting with interrupted partial runs
- Saving API costs by avoiding restarts from scratch

**Performance:** Checkpoint saves are asynchronous with < 1% throughput impact. Phase transitions use synchronous saves for data integrity.

### Concurrency Tuning

Adjust worker pool size based on your API limits:

```toml
[generation]
concurrency = 16  # More parallelism (higher throughput)
# or
concurrency = 2   # Less parallelism (lower API load)
```

**Recommendation**: Start with `concurrency = 8` and adjust based on rate limit warnings.

## Architecture

VellumForge2 follows a modular, concurrent architecture with robust error handling:

```
┌─────────────────────────────────────────────────┐
│                CLI (Cobra)                      │
│            - Flag parsing                       │
│            - Environment loading                │
└─────────────┬───────────────────────────────────┘
              │
┌─────────────▼───────────────────────────────────┐
│           Orchestrator                          │
│  - Hierarchical generation pipeline             │
│  - Smart over-generation (115% + retry)         │
│  - Worker pool management                       │
│  - Pre-validation & deduplication               │
│  - Generation statistics tracking               │
└──┬──────────┬──────────┬────────────┬───────────┘
   │          │          │            │
   ▼          ▼          ▼            ▼
┌───────┐ ┌───────┐  ┌───────┐   ┌─────────┐
│API    │ │Judge  │  │Writer │   │HF Hub   │
│Client │ │Module │  │       │   │Uploader │
│       │ │       │  │       │   │         │
│- Rate │ │- JSON │  │- Dual │   │- NDJSON │
│ limit │ │ sanit │  │  logs │   │  commit │
│- Smart│ │- Score│  │- JSONL│   │- Select │
│ retry │ │ totals│  │ writer│   │  upload │
└───────┘ └───────┘  └───────┘   └─────────┘
          │
          ▼
     ┌─────────┐
     │Validator│
     │         │
     │- Pre-   │
     │ validate│
     │- String │
     │  array  │
     │- Dedupe │
     └─────────┘
```

### Key Architectural Features

1. **Smart Over-Generation**: Requests 115% of target count, deduplicates, single retry if needed (95%+ accuracy)
2. **Pre-Validation Layer**: Validates JSON structure before unmarshaling (99%+ parse success)
3. **Robust JSON Parsing**: Proper bracket matching algorithm handles nested structures, truncated responses, and markdown-wrapped JSON
4. **Intelligent Retries**: Exponential backoff with longer delays for rate limits
5. **Dual Logging**: JSON to file for analysis + text to stdout for monitoring
6. **Selective Uploads**: Only uploads dataset and config to HF Hub (excludes logs)
7. **JSON Sanitization**: Automatically escapes literal newlines in judge responses

## Development

### Running Tests

```bash
# Run all tests
make test

# With coverage report
make test-coverage
```

### Linting & Formatting

Make sure you have golangci-lint installed.

```bash
make lint
make fmt
```

## Troubleshooting

### API Rate Limits (429 Errors)

**Symptoms**: Warnings like "Too Many Requests", generation slowing down

**Solution**: Reduce concurrency and rate limits in config:

```toml
[generation]
concurrency = 4  # Lower parallelism

[models.main]
rate_limit_per_minute = 20  # Conservative limit
```

**What VellumForge2 Does**: Automatically applies 3^n exponential backoff (6s, 18s, 54s) for rate limit retries.

### Count Mismatches

**Symptoms**: Getting fewer subtopics/prompts than requested (e.g., 271 out of 344)

**Solution**: VellumForge2 automatically handles this with **smart over-generation**:
- Requests 115% of target count initially (e.g., 395 for 344 target)
- Deduplicates results (case-insensitive)
- Makes ONE retry attempt if still short
- **Achieves 95%+ count accuracy** vs 79% with single-shot

**What you'll see in logs**:
```
INFO  Generating subtopics with over-generation strategy target=344 requesting=395 buffer_percent=15
INFO  Initial subtopic generation complete requested=395 received=390 unique=385 duplicates_filtered=5
INFO  Target count achieved final_count=344 excess_trimmed=41
```

**Note**: If retry fails, VellumForge2 gracefully returns partial results and logs a warning. Your dataset will contain the actual count generated.

### Judge Evaluation Failures

**Symptoms**: Warnings like "Judge evaluation failed: invalid character in string literal"

**Cause**: LLMs sometimes generate responses with unescaped newlines

**Solution**: VellumForge2 automatically sanitizes JSON responses. If issues persist:
- Increase `max_output_tokens` for judge model (e.g., 16384)
- Simplify judge rubric to reduce response complexity
- Check judge evaluation warnings in `session.log`

### JSON Parsing Errors

**Symptoms**: "unexpected end of JSON input" or "invalid character after array element"

**Solution**: VellumForge2 has built-in fixes:
- Auto-closes truncated JSON arrays
- Proper bracket matching ignores false matches in strings
- Extracts JSON from markdown code blocks

If errors persist:
1. Check prompt templates explicitly request JSON output
2. Ensure temperature is not too high (< 0.9 recommended)
3. Verify model supports structured output

### Out of Memory

For large datasets, reduce concurrency:

```toml
[generation]
concurrency = 2  # Minimal parallelism
```

### Empty Session Logs

If logs are empty, check file permissions on the `output/` directory.

## Generation Validation

VellumForge2 provides comprehensive validation warnings:

### Count Validation
```
WARN  Subtopic count mismatch expected=64 actual=63 difference=-1
WARN  Prompt count mismatch expected=126 actual=124 difference=-2
```

### Failure Summary
```
WARN  Generation completed with failures failure_rate=1.59% lost_rows=2
```

These warnings help diagnose issues but don't fail the entire generation. Check `session.log` for details.

## Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes with tests
4. Run linting and tests (`make lint && make test`)
5. Submit a pull request

## License

MIT License - see [LICENSE](LICENSE) file for details.

## Citation

If you use VellumForge2 in your research, please cite:

```bibtex
@software{vellumforge2,
  title = {VellumForge2: Synthetic DPO Dataset Generator},
  author = {Lamim},
  year = {2025},
  url = {https://github.com/lemon07r/vellumforge2},
  version = {1.1.0}
}
```

## Acknowledgments

Built with insights from:
- [Direct Preference Optimization paper](https://arxiv.org/abs/2305.18290)
- [sourcegraph/conc](https://github.com/sourcegraph/conc) for concurrency patterns
- The Hugging Face community for DPO training resources
- [Moonshot AI](https://www.moonshot.cn/) for the Kimi K2 Instruct model, and also inspiring the rubric style LLM as judge concept for training/datasets

## Documentation

- [Getting Started Guide](GETTING_STARTED.md) - Step-by-step tutorial
- [Changelog](CHANGELOG.md)

## Support

- [Issue Tracker](https://github.com/lemon07r/vellumforge2/issues)
- [Discussions](https://github.com/lemon07r/vellumforge2/discussions)
- Email: [Create an issue for support]

## Roadmap

VellumForge2 is currently considered feature complete, with all intended features and functionality working, and implemented. However there is still room for improvements. Feel free to fork this project to try implementing some of these improvment ideas yourselves. Some I may try to implement in the future myself, but I currently consider them pretty low priority. 

### Completed
- [x] Hierarchical generation pipeline
- [x] Concurrent worker pool
- [x] LLM-as-a-Judge evaluation
- [x] One-to-many hybrid schema
- [x] Hugging Face Hub integration (NDJSON)
- [x] Robust JSON parsing with bracket matching
- [x] Intelligent retry logic with rate limit handling
- [x] JSON sanitization for judge responses
- [x] Generation count validation
- [x] Dual logging system (JSON + text)
- [x] Story generation templates
- [x] **v1.2**: Configurable over-generation buffer
- [x] **v1.2**: Exclusion list size limits
- [x] **v1.2**: Backoff cap for rate limits
- [x] **v1.2**: Graceful shutdown (Ctrl+C)
- [x] **v1.2**: Config validation with upper bounds
- [x] **v1.2**: Template caching for performance
- [x] **v1.2**: Precompiled regex patterns
- [x] **v1.3**: Resume from checkpoint on failure (v1.3)

### Potential Ideas for Future Improvements
- [ ] Support for additional DPO schema formats
- [ ] Batch processing mode for large-scale generation
- [ ] Checkpoint compression for large datasets
- [ ] Cloud storage backends for checkpoints (S3, GCS)
- [ ] Web UI for monitoring and configuration
- [ ] Plugin system for custom judges
- [ ] Multi-model ensemble evaluation
- [ ] Export to other formats (Parquet, CSV)

---

**Status**: **STABLE** (v1.0.0)

**Status**: **RELEASE CANDIDATE** (v1.4.0) - Enhanced with Configurable Over-Generation, Graceful Shutdown, Large Performance Optimizations, Checkpoint/Resume, Async I/O, Benchmarking Tests, Agnostic Provider Support (with any OpenAI Compatible API) and CLI Management Tools


